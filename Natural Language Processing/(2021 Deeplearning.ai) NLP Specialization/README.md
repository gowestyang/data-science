This folder contains Yang Xi's study notes of **Natural Language Processing Specialization** by *deeplearning.ai*, offered on *Coursera*.

<br>

**Course Structure**

1. NLP with Classification and Vector Spaces
    * Week 1: Sentiment Analysis with Logistic Regression
        * Positive and Negative Frequency
        * Logistic Regression
    * Week 2: Sentiment Analysis with Naive Bayes
        * Naive Bayes
        * Laplacian Smoothing
        * Error Analysis
    * Week 3: Vector Space Models
        * Word-by-Word (W/W) and Word-by-Doc (W/D)
    * Week 4: Machine Translation and Document Search
        * Transformation Matrix between Different Word Embeddings
        * KNN with Locality Sensitive Hashing
<br><br>
2. NLP with Probabilistic Models
    * Week 1: Autocorrect and Minimum Edit Distance
    * Week 2: Part of Speech Tagging
        * Hidden Markov Models
        * Viterbi Algorithm
    * Week 3: N-Grams Language Model and Autocomplete
        * N-gram Probability
        * N-gram Language Model
            * Training, Evaluation and Perplexity
            * Out of Vocabulary (OOV), Smoothing, Backoff, Interpolation
    * Week 4: Word Embeddings by Continuous Bag of Words (CBOW) Model
<br><br>
3. NLP with Sequence Models
    * Week 1: Neural Networks for Sentiment Analysis
        * Introduction to neural networks and Trax
    * Week 2: Recurrent Neural Networks (RNN) for Language Modeling
        * Perplexity
        * Gated Recurrent Units (GRU)
        * Bi-directional RNN and Deep RNN
    * Week 3: Long Short Term Memory (LSTM) and Named Entity Recognition (NER)
    * Week 4: Siamese Networks and Question Duplicates
        * Triplet Loss Function, Hard Negative Mining (Mean Negative, Closest Negative)
        * One Shot Learning
<br><br>
4. NLP with Attention Models
    * Week 1: Neural Machine Translation (NMT) with Attention
        * Seq2Seq, the Information Bottleneck
        * Attention, alignment score, context vector
        * Transformer, Queries, Keys, Values, Scaled Dot-Product Attention
        * Teacher Forcing, Curriculum Learning
        * Bi-Lingual Evaluation Understudy (BLEU), Recall-Oriented Understudy of Gisting Evaluation (ROUGE), F1
        * Greedy Decoding, Random Sampling, Beam Search, Minimum Bayer Risk
        * Subword Tokenization, Bucketing
        * Trax Select and Residual Layers
    * Week 2: Text Summarization with Transformers
        * Multi-Head Attention, Encoder-Decoder Attention, Bi-directional Self-attention, Masked Self-Attention (Causal Attention)
        * Positional Embedding
        * GPT-2, BERT, T5
        * Summarization: Training and Inferencing
    * Week 3: Question Answering with BERT and T5
        * Transfer Learning (TL)
        * BERT, (Multi-)Masked Language Modeling, Next Sentence Prediction (NSP), Segment Embedding, Fine-tuning BERT
        * T5, Encoder-Decoder Attention, Multi-Task Training, Text-to-Text, Temperature Scaled Mixing
        * General Language Understanding Evaluation (GLUE) Benchmark, Model Agnostic
        * Question Answering (Context-based, Closed-book)
        * Tokenization, SentencePiece, NFKC, Byte Pair Encoding (BPE)
        * Hugging Face
    * Week 4: Chatbot
        * Reformer, Locality Sensitive Hashing (LSH) Attention, Reversible Residual Layers
