{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning\n",
    "Yang Xi <br>\n",
    "28 Sep, 2020\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretable machine learning are generally achieved from three approaches:\n",
    "1.\tUse an **interpretable (or transparent) model**, such as decision tree, generalized linear model\n",
    "    * Note that, even a interpretable model can be hard to interpret. For example, when a linear model has too many coefficients.<br><br>\n",
    "2.\tUse **model-specific explanations**, such as certain methods for random forests or artificial neural networks.<br><br>\n",
    "3.\tUse **model-independent (or model-agnostic) explanations**. Such method can explain the predictions of any model, and it is easier to compare two candidate models or to switch between different types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Independent Methods\n",
    "An ideal model explainer should posses the following properties:\n",
    "* Interpretable\n",
    "* Model Agnostic\n",
    "* Local Fidelity: explain what leads to the prediction at a specific instance\n",
    "* Global Perspective: present a global intuition of the model\n",
    "\n",
    "There are three main types of model-agnostic explanation methods:\n",
    "1.\t[**Local Interpretable Model-agnostic Explanations (LIME)**](http://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf): Generate explanations using the components of an interpretable model, which approximates the black-box model locally around the point of interest.<br><br>\n",
    "\n",
    "2.\t**Explanation Vectors**: Define explanations as gradient vectors at the point of interest, which characterize how a data point has to be moved to change its prediction.<br><br>\n",
    "\n",
    "3. [**Interactions-Based Method for Explanation (IME)**](http://www.jmlr.org/papers/volume11/strumbelj10a/strumbelj10a.pdf): Based on cooperative game theory, this method considers features as players of a game. It divides the total change in prediction among the features, in a way that is “fair” to their contributions across all possible subsets of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability Does Not Mean Causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[(2018 Pol) Interpretable Machine Learning: An Overview](https://becominghuman.ai/interpretable-machine-learning-an-overview-10684eaa1fd7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
