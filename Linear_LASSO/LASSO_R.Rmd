---
title: "LASSO (R)"
author: "Yang Xi"
date: "14 Nov, 2018"
output:
  html_document:
    toc: 1
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

<br>

# Example: Classification with Imbalanced Class
```{r, message=FALSE}
library(tidyverse)
library(caret)
library(MLmetrics)

dfTrain0 <- read_csv("../../data/(2016 UCI) Credit Default/data_train.csv")
dfTest0 <- read_csv("../../data/(2016 UCI) Credit Default/data_test.csv")

```

<br>

**Preprocessing**
LASSO by `glmnet` package has three requirements:<br>

* predictors are standardized
    + implemented by `glmnet()` by default, and coefficients are returned on the original scale.
* factors in predictors are converted to dummy variables
* factors in target variable meet R naming (i.e. cannot be named as 0, 1)
```{r}
PrepTrainTest <- function(df) {
  df %>%
    mutate(Education = relevel(as.factor(Education), ref="high school"),
           SepRepayment = relevel(as.factor(SepRepayment), ref="paid"),
           Default = as.factor(ifelse(Default==0, "N", "Y")))
}
dfTrain <- PrepTrainTest(dfTrain0)

dummyTrain <- dummyVars(Default ~ ., data=dfTrain, fullRank=T)
XTrain <- predict(dummyTrain, dfTrain)

## following standardization is not needed as glmnet implements standardization by default
## run these code will not vary the results
# preProcTrain <- preProcess(XTrain) # center and scale by default
# XTrain <- predict(preProcTrain, XTrain)

yTrain <- dfTrain$Default
yWeights <- ifelse(yTrain=="N", 1, sum(yTrain=="N")/sum(yTrain=="Y"))
```

<br>

### Tune Optimal `lambda` Through Cross-Validation
This code uses `tuneGrid` from `caret`.<br>
`cv.glmnet` can be 4-5 times faster, but it does not support f1 measure.<br>
This code does not implement one-standard-error (1SE) rule.
```{r}
f1 <- function(data, lev = NULL, model = "F1") {
  c(F1 = F1_Score(data$obs, data$pred, positive = lev[2]))
}
lassoFit <- train(XTrain, yTrain, method="glmnet",
                  weights = yWeights,
                  metric = "F1",
                  trControl = trainControl(method = "cv", # 10 fold by default
                                           summaryFunction = f1),
                  tuneGrid = expand.grid(alpha = 1,
                                         lambda = seq(0.001,0.1,by = 0.001)
                                                      ))

# optimal paraters and CV performance
tuneResults <- lassoFit$results
ggplot(tuneResults, aes(x=lambda, y=F1)) + geom_line() + theme_minimal()

filter(tuneResults, lambda==lassoFit$bestTune$lambda)

```

<br>

### Model Interpretation
```{r}
# Note that glmnet() returns coefficients in original scale
coef(lassoFit$finalModel, lassoFit$bestTune$lambda)
```

<br>

### Train Performance
```{r}
probTrain <- predict(lassoFit, type="prob")$Y
predTrain <- predict(lassoFit)

cmTrain <- confusionMatrix(predTrain, yTrain, positive="Y")
cmTrain

perfTrain <- data.frame(F1 = round(cmTrain$byClass['F1'], 3),
                        AUC = round(AUC(probTrain,
                                        plyr::revalue(yTrain, c("N"="0", "Y"="1"))),
                                    3))
row.names(perfTrain) <- NULL
perfTrain
```

<br>

### Test Performance
```{r}
dfTest <- PrepTrainTest(dfTest0)
XTest <- predict(dummyTrain, dfTest)

predTest <- predict(lassoFit, XTest)

cmTest <- confusionMatrix(predTest, dfTest$Default, positive="Y")
cmTest

paste('Test F1 score is', round(cmTest$byClass['F1'], 3))
```

<br>



