# 6.1 Motivation
Explain what makes ML ensemble methods effective, and how to avoid common errors when applying them in finance.

<br>

# 6.2 The 3 Sources of Errors
* Bias
    * Error caused by unrealistic assumptions
    * High bias leads to underfitting
* Variance
    * Error caused by sensitivity to small changes in training set
    * High variance lead to overfitting
    * Mistaken noise with signal
* Noise
    * Caused by variance of the observed values
    * Unpredictable, irreducible, cannot be explained by any model

An ensemble method combines a set of weak learners to create a stronger learner, which reduces bias and/or variance.

<br>

# 6.3 Bootstrap Aggregation (Bagging)
Bagging is an effective way of reducing the variance
* Sample training datasets with replacement
* Fit weak learner in parallel
* Ensemble with average or proportion

<br>

### 6.3.1 Variance Reduction
For bagging to work well, the samples should be as independent as possible

<br>

### 6.3.2 Improved Accuracy
The bagging classifier's accuracy **exceeds the average accuracy** of the individual classifiers
* In favor of bagging any classifier in general
* Cannot improve the accuracy of poor classifiers - if individual learns are poor, the result of bagging will still be poor - bagging is effective in reducing variance but not bias.

<br>

### 6.3.3 Observation Redundancy
Usually financial observations cannot be assumed to be IID. This caused redundant observations, which has two detrimental effects on bagging:
* Samples drawn with replacement are more likely to be virtually identical
    * Secion 4.5 recommended three alternative solutions
    * A better solution is to apply **sequential bootstrap**
* Out-of-bag accuracy will be inflated
    * By random sampling with replacement, the training set and out-of-bag set are very similar
    * Will result in much lower accuracy in a proper **stratified k-fold cross-validation without shuffling before partitioning**
    * Prefer low number of folds to avoid excessive partitioning

<br>

# 6.4 Random Forest
RF produces ensemble of decision trees with low variance.
* Another level of randomness on top of bagging: at each node split, evaluate only a subsample (without replacement) of attributes (features)
* Benefits
    * Reduced variance
    * Evaluate feature importance
    * Out-of-bag accuracy estimates -- be aware that they are likely to be inflated in financial applications

Overfitting will still take place if large number of samples are redundant. Some ways to address overfitting:
* Use a smaller `max_features` to force discrepancy between trees
* Early stopping: use a large (e.g., 5%) `min_weight_fraction_leaf`, or smaller `max_depth`, such that out-of-bag accuracy converges to out-of-sample accuracy.
* Set `max_samples` to average uniqueness between samples `avgU`
* Replace standard bootstrapping with sequential bootstrapping

Additional suggestions:
* Fit RF on a PCA of the features, because fitting decision tress in feature space with direction aligned with axes typically reduces the number of levels needed.
* Use `balanced_subsample` for `class_weight`

<br>

# 6.5 Boosting
Boosting combines weak estimators to achieve one with high accuracy.
* Steps
    * Sample with replacement with some sample weights (initialized with uniform weights)
    * Fit an estimator
    * Evaluate the accuracy of the estimator - if passed an acceptance threshold, keep the estimator, otherwise discard it
    * Give more weight to misclassified observations, and less weight to correctly classified observations
    * Repeat above until N estimators are produced
    * Ensemble prediction as the weighted average of the individual forecasts from the N estimators
        * The weight is determined by the accuracy of the individual estimator

<br>

# 6.6 Bagging vs. Boosting in Finance
Several aspects of boosting:
* Fit individual estimators sequentially
* Dismiss poor-performing classifiers
* Weight observations differently in each iteration
* Use weighted average to ensemble

Boosting reduces both variance and bias, at the cost of greater risk of overfitting.

It could be argued that in financial applications, **bagging is generally preferable to boosting**.
* Bagging addresses overfitting, while boosting addresses underfitting
* Overfitting is often a greater concern
* Bagging can be parallelized

<br>

# 6.7 Bagging for Scalability
Some ML algorithms do not scale well with sampel size. One solution is to use bagging.
* It is also preferred to impose a tight early stopping condition for the base estimator (set a low max internation, or a high tolerance level)




