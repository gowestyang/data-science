# Feature Transformation and Model Interpretation

Yang Xi<br>
30 Sep 2020

<br>

* Feature Transformation as Part of Model Pipeline
    * Pre-processing Methods
* Model Interpretation
* References

<br>

<br>

## Feature Transformation as Part of Model Pipeline

Feature transformation (pre-processing) is sometimes described as an individual task.<br>
While here I put it as part of the model training process, to emphasize two important aspects:
* What transformation to carry out is dependent on the model.
* Feature transformation should be "encupsulate" together with the model in a single **pipeline**:
    * One reason it that you have to do this during **model deployment**.
    * Another important reason is to address the risk of **information leakage**

<br>

**Data leakage** is a well-known issue in machine learning project, where a model is trained on features which could not be obtained during inference.

From a different perspective, during model training, it is tempting to transform the features first before splitting the data. Such operation could cause **information leakage**.<br>
For example, when transforming features using methods like standardization, multi-variate imputation, tokenization, you are using information from the dev/test data set.

The **result can be severe**: the model will perform well in training, dev and test - then fail in the real world.

<br>

### Pre-processing Methods
In this section, I will summarize some basic method of pre-processing.<br>
While it still depends on the model or objective whether certain method is needed. 
* Tabular data
    * Outlier adjustment
    * Missing value imputation
    * Skewness adjustment
    * Standardization
    * Dimensional reduction
* Text data
    * Tokenization
    * Normalization (lowercase, special characters, numbers, stopwords, etc)
    * Lemmatization (sometimes also user dictionary)
    * TF-IDF filter
    * Word2Vec
    * Feature hashing
* Time-Series
    * Box-cox transformation for variance stabilization
    * Differencing for mean stabilization

<br>

## Model Interpretation

Interpreting a trained model and the prediction results is also important to understand the model behavior, as well as to gain confidence and trust on the model.<br>
**Interpretable machine learning** are generally achieved from three approaches:
1.	Use an **interpretable (or transparent) model**, such as decision tree, generalized linear model
    * Note that, even a interpretable model can be hard to interpret. For example, when a linear model has too many coefficients.
2.	Use **model-specific explanations**, such as certain methods for random forests or artificial neural networks.
3. Use **model-independent (or model-agnostic) explanations**. Such method can explain the predictions of any model, and it is easier to compare two candidate models or to switch between different types of models.

<br>

### Model-Independent Methods
An ideal model explainer should posses the following properties:
* Interpretable
* Model Agnostic (model-independent)
* Local Fidelity: explain what leads to the prediction at a specific instance
* Global Perspective: present a global intuition of the model

There are three main types of model-agnostic explanation methods:
1.	[**Local Interpretable Model-agnostic Explanations (LIME)**](http://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf): Generate explanations using the components of an interpretable model, which approximates the black-box model locally around the point of interest.

2.	**Explanation Vectors**: Define explanations as gradient vectors at the point of interest, which characterize how a data point has to be moved to change its prediction.

3. [**Interactions-Based Method for Explanation (IME)**](http://www.jmlr.org/papers/volume11/strumbelj10a/strumbelj10a.pdf): Based on cooperative game theory, this method considers features as players of a game. It divides the total change in prediction among the features, in a way that is “fair” to their contributions across all possible subsets of features.

Last but not least, **interpretability does not mean causality**.

<br>

## References
* [(scikit-learn) Pipelines and Composite Estimators](https://scikit-learn.org/stable/modules/compose.html)
* [(2018 Rob and George) Forecasting: Principles and Practice](https://otexts.com/fpp2/)
* [(2018 Pol) Interpretable Machine Learning: An Overview](https://becominghuman.ai/interpretable-machine-learning-an-overview-10684eaa1fd7)
