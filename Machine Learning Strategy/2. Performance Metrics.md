# Performance Metrics

Yang Xi<br>
30 Sep 2020

<br>

* Orthogonalization of Performance Metrics
* Single Number Performance Metric
* Cost of Error
* Performance Benchmark
* Unsupervised Learning
* References

<br>

<br>

## Orthogonalization of Performance Metrics
I usually see people thinking about model performance metrics and which model to use at the same time.<br>
While with the orthogonalization mindset, we should first define the performance metrics which aligns with the business objective; then worry separately about which model to use and how to do well on this metric.

<br>

## Single Number Performance Metric
It is recommended to use a single performance metric, which will make the decision making process smoother, and speed up the model iteration.

However, most business problems have concerns around multiple metrics, which may also form a "trade-off" (revenue - profit, precision - recall). One strategy could be setting up one or more satisfying metrics and one optimizing metric:
* **satisfying metics**: acceptable as long as the performance passed a defined threshold
* **optimizing metric**: the metric to be aligned with the cost function.

It is important to make sure that the performance metric is aligned with the end users preference.<br>
For example, we have two models to pick cat photos:
* We use classification error as the metric.
* Model A: 3% error, but it sometimes pick pornographic photos.
* Model B: 5% error, but no pornographic photo.

In this case, we need to re-define the performance metric, i.e., place higher punishment on pornographic photos.

<br>

### Cost of Error
To define the performance metrics, it is useful to understand what is the business cost of the output error.

For example, in a binary classification problem, if we know that
* a Type I error (false positive or false alarm) will cost $10
* a Type II error (false negative or miss) will cost $2

We would focus more on reducing false alarm, or placing more weight on precision.

While it's usually hard for business to quantify the cost of error, we should at least make an approximate or assumption, which will guide our design of performance metrics.

<br>

## Performance Benchmark
It usually useful to compare the model performance against to a benchmark.<br>
The benchmark could be:
* An existing model
* A simple model
* Random guess
    * Note that for time-series forecasting, the random guess should be 1-step-naive or 1-step-seasonal-naive

<br>

## Unsupervised Learning
In an unsupervised learning problem, it's generally more difficult to define the goodness of the model.<br>
Regardless, we should still develop performance metrics which align with the business problem.<br>
The several thoughts could probably help:
* Is there any specific pattern expected by the stakeholders?
    * If yes, then the model should at least capture such pattern (this is a satisfying metric).
* Should the pattern persist over time?
    * Usually yes - the model should be interpreted in a consistent manner over time.
* Should the model capture transit patterns?
    * This is a trade-off of the previous point.

<br>

## References
* [(2020 Andrew Ng) Structuring Machine Learning Projects](https://www.coursera.org/learn/machine-learning-projects)
* [(2013 Foster) Data Science for Business](http://www.data-science-for-biz.com/)
* [(2018 Rob and George) Forecasting: Principles and Practice](https://otexts.com/fpp2/)
